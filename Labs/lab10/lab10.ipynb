{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly Recommended: Use Google Colab to Avoid Errors Related to 3D plots rendering.\n",
    "\n",
    "Make sure you have plotly installed (pip install plotly) and numpy installed as well if using local IDEs and set pio.renderers.default to 'browser' in each cell. If using jupyter application, set pio.renderers.default to 'jupyter'. No change is needed if using Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN THE FOLLOWING, CELL COMPLETE THE TODOs TO CODE LINEAR REGRESSION AND GRADIENT DESCENT FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface min cost: 3.789140\n",
      "Gradient descent cost: 3.786755\n",
      "Found at: w=3.0070, b=7.0243\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Generate 1D synthetic linear data\n",
    "np.random.seed(4)\n",
    "x = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 3 * x + 7 + np.random.normal(0, 2, size=(100, 1))\n",
    "\n",
    "# TO DO: Cost function (MSE for linear regression)\n",
    "# First calculate the y_hat (predicted values) and then average difference of actual and predicted values across all points\n",
    "def compute_cost(w, b):\n",
    "    y_hat = w * x + b\n",
    "    return np.mean((y - y_hat) ** 2)\n",
    "\n",
    "# TO DO: Manual gradient descent to find optimal w, b\n",
    "def gradient_descent_path(x, y, lr=0.01, epochs=2000):\n",
    "    # TO DO: Complete this loop of gradient descent. You can initialize weights and bias with 0.0\n",
    "    # NOTE THAT: save w, b, and cost for each epoch, and return them as 3 arrays. This will be used in next step to visualize path of gradient descent.\n",
    "\n",
    "    w = 0.0\n",
    "    b = 0.0\n",
    "\n",
    "    w_path = []\n",
    "    b_path = []\n",
    "    cost_path = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_hat = w*x + b \n",
    "        error = y_hat - y\n",
    "\n",
    "        dw = np.mean(error * x)\n",
    "        db = np.mean(error)\n",
    "\n",
    "        w -= lr * dw \n",
    "        b -= lr * db \n",
    "\n",
    "        cost = np.mean((y - (w * x + b)) ** 2)\n",
    "        w_path.append(w)\n",
    "        b_path.append(b)\n",
    "        cost_path.append(cost)\n",
    "\n",
    "    return np.array(w_path), np.array(b_path), np.array(cost_path)\n",
    "\n",
    "\n",
    "# Grid for (w, b)\n",
    "w_range = np.linspace(-2, 6, 200)\n",
    "b_range = np.linspace(0, 14, 200)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "Z = np.zeros_like(W)\n",
    "    \n",
    "\n",
    "# Compute cost surface\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        Z[i, j] = compute_cost(W[i, j], B[i, j])\n",
    "\n",
    "w_path, b_path, cost_path = gradient_descent_path(x, y)\n",
    "w_opt, b_opt, optimal_cost = w_path[-1], b_path[-1], cost_path[-1]\n",
    "\n",
    "# Plotting interactive cost plot\n",
    "fig = go.Figure(data=[\n",
    "    go.Surface(z=Z, x=W, y=B, colorscale='Viridis', opacity=0.85),\n",
    "    go.Scatter3d(\n",
    "        x=[w_opt],\n",
    "        y=[b_opt],\n",
    "        z=[optimal_cost],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color='red'),\n",
    "        name='Optimal (w, b)'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Interactive Cost Surface for Linear Regression (MSE)\",\n",
    "    scene=dict(\n",
    "        xaxis_title='w',\n",
    "        yaxis_title='b',\n",
    "        zaxis_title='MSE Cost'\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "pio.renderers.default = 'browser'  # or 'jupyter' or 'browser'\n",
    "fig.show()\n",
    "\n",
    "# Print cost comparison\n",
    "print(f\"Surface min cost: {Z.min():.6f}\")\n",
    "print(f\"Gradient descent cost: {optimal_cost:.6f}\")\n",
    "print(f\"Found at: w={w_opt:.4f}, b={b_opt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUST RUN THE FOLLOWING CELL TO OBSERVE GRADIENT DESCENT PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface min cost: 3.789140\n",
      "Gradient descent cost: 3.786755\n",
      "Found at: w = 3.0070, b = 7.0243\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "\n",
    "# Surface grid\n",
    "w_range = np.linspace(-2, 6, 200)\n",
    "b_range = np.linspace(0, 14, 200)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "Z = np.zeros_like(W)\n",
    "\n",
    "# Compute cost surface\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        Z[i, j] = compute_cost(W[i, j], B[i, j]) # compute_cost() from previious cell\n",
    "\n",
    "\n",
    "w_path, b_path, cost_path = gradient_descent_path(x, y) # gradient_descent_path() from previous cell\n",
    "\n",
    "# Plot interactive surface and ball path\n",
    "fig = go.Figure(data=[\n",
    "    go.Surface(z=Z, x=W, y=B, colorscale='Viridis', opacity=0.85),\n",
    "    go.Scatter3d(\n",
    "        x=w_path,\n",
    "        y=b_path,\n",
    "        z=cost_path,\n",
    "        mode='lines+markers',\n",
    "        marker=dict(size=3, color='red'),\n",
    "        line=dict(color='red', width=2),\n",
    "        name='Gradient Descent Path'\n",
    "    ),\n",
    "    go.Scatter3d(\n",
    "        x=[w_path[-1]],\n",
    "        y=[b_path[-1]],\n",
    "        z=[cost_path[-1]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color='black'),\n",
    "        name='Final Point'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Gradient Descent Path on Linear Regression MSE Surface\",\n",
    "    scene=dict(\n",
    "        xaxis_title='w',\n",
    "        yaxis_title='b',\n",
    "        zaxis_title='Cost',\n",
    "        camera=dict(eye=dict(x=1.3, y=1.3, z=0.7)),\n",
    "        xaxis=dict(range=[-2, 6]),\n",
    "        yaxis=dict(range=[0, 14]),\n",
    "        zaxis=dict(range=[0, max(cost_path)])\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Set renderer and show plot\n",
    "pio.renderers.default = 'browser'  # or 'jupyter' or 'browser'\n",
    "fig.show()\n",
    "\n",
    "# Final metrics\n",
    "print(f\"Surface min cost: {Z.min():.6f}\")\n",
    "print(f\"Gradient descent cost: {cost_path[-1]:.6f}\")\n",
    "print(f\"Found at: w = {w_path[-1]:.4f}, b = {b_path[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW, LET US OBSERVE NLL FOR LINEAR REGRESSION INSTEAD OF MSE LOSS.  \n",
    "JUST RUN THE FOLLOWING CELL TO SEE THE CORRESPONDING MSE AND NLL CURVES AND ANSWER THE QUESTIONS IN NEXT CELL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Generate linear regression data\n",
    "np.random.seed(4)\n",
    "x = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 3 * x + 7 + np.random.normal(0, 2, size=(100, 1))\n",
    "\n",
    "# Cost functions\n",
    "def compute_cost(w, b, loss='nll'):\n",
    "    y_hat = w * x + b\n",
    "    eps = 1e-8\n",
    "    if loss == 'nll':\n",
    "        residuals = y - y_hat\n",
    "        sigma_sq = np.var(residuals)\n",
    "        return 0.5 * np.mean((residuals ** 2) / (sigma_sq + eps) + np.log(2 * np.pi * (sigma_sq + eps)))\n",
    "    elif loss == 'mse':\n",
    "        return np.mean((y - y_hat)**2)\n",
    "\n",
    "# Grid for (w, b)\n",
    "w_range = np.linspace(-2, 6, 200)\n",
    "b_range = np.linspace(0, 14, 200)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "Z_nll = np.zeros_like(W)\n",
    "Z_mse = np.zeros_like(W)\n",
    "\n",
    "# Compute both surfaces\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        Z_nll[i, j] = compute_cost(W[i, j], B[i, j], loss='nll')\n",
    "        Z_mse[i, j] = compute_cost(W[i, j], B[i, j], loss='mse')\n",
    "\n",
    "# Plot NLL\n",
    "fig_nll = go.Figure()\n",
    "fig_nll.add_trace(go.Surface(z=Z_nll, x=W, y=B, colorscale='Viridis'))\n",
    "fig_nll.update_layout(\n",
    "    title=\"NLL Cost Surface (Linear Regression - Gaussian Likelihood)\",\n",
    "    scene=dict(xaxis_title='w', yaxis_title='b', zaxis_title='NLL Cost'),\n",
    "    width=800, height=600\n",
    ")\n",
    "fig_nll.show()\n",
    "\n",
    "# Plot MSE\n",
    "fig_mse = go.Figure()\n",
    "fig_mse.add_trace(go.Surface(z=Z_mse, x=W, y=B, colorscale='Plasma'))\n",
    "fig_mse.update_layout(\n",
    "    title=\"MSE Cost Surface (Linear Regression)\",\n",
    "    scene=dict(xaxis_title='w', yaxis_title='b', zaxis_title='MSE Cost'),\n",
    "    width=800, height=600\n",
    ")\n",
    "fig_mse.show()\n",
    "\n",
    "pio.renderers.default = 'browser' # or 'jupyter' or 'browser'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTIONS:\n",
    "1. How is the NLL curve different from the MSE curve?\n",
    "\n",
    "    The MSE curve is more smooth while the NLL is sharper and steeper near the center.\n",
    "    \n",
    "2. What potential problems can this NLL curve cause and how would they affect the gradient descent process?\n",
    "\n",
    "    The steep curves can cause large gradiate steps which might make the algo overshoot. The MSE curve is more stable, which makes it easier for gradient descent to find a global minimum.\n",
    "\n",
    "3. WHICH ONE IS BETTER FOR LINEAR REGRESSION (OBSERVE THE CURVES AND ANSWER)?\n",
    "\n",
    "    MSE is better for linear regression since it is smooth and predictable. It offers a stable convergence and has only one global minimum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
