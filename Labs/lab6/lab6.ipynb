{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Comparing kNN Classification Capabilities with 3 Data Models\n",
    "\n",
    "- Bag of Words\n",
    "- TF-IDF\n",
    "- TF-IDF with LSA\n",
    "\n",
    "Dependency Installation: `pip install scikit-learn matplotlib numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Feature Extraction Function\n",
    "\n",
    "This function takes a list of text documents and returns the the bag of words matrix using the `CountVectorizer` class from `scikit-learn`.\n",
    "\n",
    "An explanation of how Bag of Words is calculated:\n",
    "\n",
    "- Build a vocabulary from the collection of documents (limiting to a maximum number of features)\n",
    "\n",
    "- For each document, represent it as a vector where each element is the count of a specific term from the vocabulary\n",
    "\n",
    "- The resulting matrix has dimensions `m x n`, where `m` is the number of documents and `n` is the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bow(documents, max_features):\n",
    "    \"\"\"\n",
    "    Computes the bag-of-words matrix for the given documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents: List of text documents.\n",
    "    - max_features: Maximum number of features (vocabulary size).\n",
    "    \n",
    "    Returns:\n",
    "    - bow_matrix: Sparse matrix of shape (n_samples, n_features).\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    bow_matrix = vectorizer.fit_transform(documents)\n",
    "    return bow_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Feature Extraction\n",
    "\n",
    "This function takes a list of text documents and returns the tfidf-weighted term-document matrix, using the `TfidfVectorizer` calss from `scikit-learn` to perform the calculations described below.\n",
    "\n",
    "A reminder on how TF-IDF matrices are calculated:\n",
    "\n",
    "- For each term `t` in document `d`, compute the term frequency: `tf(t, d)` = frequency of `t` in `d`\n",
    "\n",
    "- Compute the inverse document frequency over all `N` documents: `idf(t) = log((N + 1) / (df(t) + 1)) + 1`, where `df(t)` is the number of documents containing `t`\n",
    "\n",
    "- Calculate the TF-IDF weight for each term-document pair: `tfidf(t, d) = tf(t, d) * idf(t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(documents, max_features):\n",
    "    \"\"\"\n",
    "    Computes the tfidf matrix for the given documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents: List of text documents.\n",
    "    - max_features: Maximum number of features to use.\n",
    "    \n",
    "    Returns:\n",
    "    - tfidf_matrix: Sparse matrix of shape (n_samples, n_features).\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Rank Approximation via Latent Semantic Analysis (LSA)\n",
    "\n",
    "This function uses the `TruncatedSVD` class from `scikit-learn` to reduce the dimensionality of the input tfidf matrix.\n",
    "\n",
    "A reminder on how LSA is performed:\n",
    "\n",
    "- Start with the TF-IDF matrix `A` of size `m x n` (with `m` documents and `n` terms)\n",
    "\n",
    "- Apply Singular Value Decomposition (SVD): `A = U * Σ * Vᵀ`\n",
    "\n",
    "- Create a low-rank approximation by retaining the top `k` singular values and corresponding vectors: `A ≈ Uₖ * Σₖ * Vₖᵀ`, where `Σₖ` is a `k x k` diagonal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lsa(tfidf_matrix, n_components):\n",
    "    \"\"\"\n",
    "    Applies LSA (using TruncatedSVD) to the tfidf matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - tfidf_matrix: Sparse matrix from tfidf vectorization.\n",
    "    - n_components: Number of components to keep.\n",
    "    \n",
    "    Returns:\n",
    "    - lsa_matrix: Dense matrix with reduced dimensions.\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
    "    return lsa_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Classification\n",
    "\n",
    "This function trains a kNN classifier on the input features using the `KNeighborsClassifier` class from `scikit-learn` and calculates its acurracy.\n",
    "\n",
    "A reminder on how KNN training is performed:\n",
    "\n",
    "- Represent each document by its feature vector (from either the TF-IDF matrix or the LSA-reduced matrix)\n",
    "\n",
    "- Compute the Euclidean distance between vectors: `d(x, xᵢ) = || x - xᵢ ||₂`\n",
    "\n",
    "- For a given document, identify the `k` nearest neighbors based on these distances\n",
    "\n",
    "- Assign the document to the class most common among these `k` neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_knn(features, labels, n_neighbors=5, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the dataset, trains a kNN classifier, and evaluates performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - features: Feature matrix (can be tfidf or LSA-transformed).\n",
    "    - labels: Ground truth labels.\n",
    "    - n_neighbors: Number of neighbors for kNN.\n",
    "    - test_size: Fraction of data to use for testing.\n",
    "    \n",
    "    Returns:\n",
    "    - accuracy: Accuracy score on the test set.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size)\n",
    "\n",
    "    neigh = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    neigh.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = neigh.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data\n",
    "\n",
    "- 20newsgroups dataset (see here for more [info](https://www.kaggle.com/datasets/crawford/20-newsgroups))\n",
    "\n",
    "  - Text which is categorized into 20 different categories\n",
    "\n",
    "  - `newsgroups.data` contains the text data\n",
    "\n",
    "  - `newsgroups.target` contains the corresponding categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 20 Newsgroups dataset (using all available data for a robust comparison)\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, \n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "documents = newsgroups.data\n",
    "labels = newsgroups.target\n",
    "num_features = 10000 # Not too many for speed\n",
    "num_components = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply All 3 Data Models with kNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating kNN classifier on Bag-of-Words features: 0.45 acurracy\n",
      "\n",
      "Evaluating kNN classifier on TF-IDF features: 0.32 acurracy\n",
      "\n",
      "Evaluating kNN classifier on LSA-transformed features: 0.68 acurracy\n"
     ]
    }
   ],
   "source": [
    "# --- Pipeline 1: Bag-of-Words ---\n",
    "bow_matrix = compute_bow(documents, num_features)\n",
    "acc_bow = evaluate_knn(bow_matrix, labels)\n",
    "print(f\"\\nEvaluating kNN classifier on Bag-of-Words features: {acc_bow:.2f} acurracy\")\n",
    "\n",
    "\n",
    "# --- Pipeline 2: TF-IDF ---\n",
    "tfidf_matrix = compute_tfidf(documents, num_features)\n",
    "acc_tfidf = evaluate_knn(tfidf_matrix, labels)\n",
    "print(f\"\\nEvaluating kNN classifier on TF-IDF features: {acc_tfidf:.2f} acurracy\")\n",
    "\n",
    "\n",
    "# --- Pipeline 3: TF-IDF + LSA ---\n",
    "lsa_matrix = apply_lsa(tfidf_matrix, num_components)\n",
    "acc_lsa = evaluate_knn(lsa_matrix, labels)\n",
    "print(f\"\\nEvaluating kNN classifier on LSA-transformed features: {acc_lsa:.2f} acurracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehension Questions\n",
    "\n",
    "1.\tWhy does applying LSA to the TF-IDF matrix significantly improve the kNN classifier’s performance compared to using raw TF-IDF or Bag-of-Words features?\n",
    "\n",
    "It helps by capturing the underlying relationships between the words and documents. SVD is used to reduce dimensionality and gropup similar terms together, which can help knn perform better.\n",
    "\n",
    "2.\tGiven that the TF-IDF approach resulted in lower accuracy than the Bag-of-Words model in this example, what factors could be contributing to this outcome, and what insights does it offer about the weighting of terms in text classification?\n",
    "\n",
    "TF-IDF reduces the weight for the frequent words, giving more importance to rare ones. This might not be good for kNN since reducing the influence of frequent words can cause incorrect nearest neighbor matches. It tells us that term weighting affects text classification performance, and depending how what is used, different weighing approaches might be more effective. In the case of TF-IDF and kNN, it wasn't as helpful in improving the accuracy in the final model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
